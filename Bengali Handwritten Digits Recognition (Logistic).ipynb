{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "160204011_problem1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0641Q1HBwt8W"
      },
      "source": [
        "###NumtaDB Bangla handwritten Digits\n",
        "Bengali Handwritten Digits recognization can be solved using many techniques.  I will use logistic regression for solving this problem\n",
        " \n",
        "the main motive is to make the model recognize the hand written digit accurately.,changing the hyperparameter to see for which combination the accuracy is better.\n",
        "\n",
        "\n",
        "**Numta DB** is a database.This database contains handwritten digits (0 through 9) in bengali, and can provide a baseline for testing image processing systems.\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1k5iKYOTrJmuSRPbWazZyBboaUPWabcmR\" width=\"400\">\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0ZO5nuUYO41"
      },
      "source": [
        "**Importing all necessary library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5pshGFhEmFy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from os import path\n",
        "from torchvision import  models\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grCZpAJqExjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1cd6c2-2511-46de-9f21-862a11f8f8ec"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.10)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "lhQeA6c3E1DK",
        "outputId": "06da5350-7c18-4d2e-cae9-615544d3e16d"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c947443-8d02-4788-92ad-420a897fa6e3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8c947443-8d02-4788-92ad-420a897fa6e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"saimom\",\"key\":\"6658d2c401a0700e06b404d8140ba08f\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhFaN-h9E4zB",
        "outputId": "e0432f9c-2315-4b73-ac4a-265fec866c70"
      },
      "source": [
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLWAbMwYcSn"
      },
      "source": [
        "**importing numtaDB dataset from kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrXfOM6AJclC",
        "outputId": "85168392-5822-4d9c-9382-744da6d18616"
      },
      "source": [
        "!kaggle datasets download -d BengaliAI/numta -p /content/gdrive/My\\ Drive/Softcom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading numta.zip to /content/gdrive/My Drive/Softcom\n",
            "100% 1.91G/1.91G [00:17<00:00, 34.1MB/s]\n",
            "100% 1.91G/1.91G [00:17<00:00, 115MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP_7dIwzYnXB"
      },
      "source": [
        "**unzipping numta.zip dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56_Z7kv7F2hL",
        "outputId": "6c0d4b17-863f-4caf-856d-cab1b61c00d7"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"/content/gdrive/My Drive/Softcom/numta.zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkDp94RSGHok",
        "outputId": "40f4633c-a748-4296-c259-131e38517691"
      },
      "source": [
        "from google.colab import drive #mounted all file and folder from google drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0yg4xSlZzGL"
      },
      "source": [
        "**changing the directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2c81uYEHL76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b23930-3832-46f4-a95c-909fc971bd40"
      },
      "source": [
        "root_path = '/content/gdrive/My Drive/Softcom/'\n",
        "Root = '/content/'\n",
        "os.listdir(Root1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['debugger_yzyhwhgxp',\n",
              " 'dap_multiplexer.INFO',\n",
              " 'initgoogle_syslog_dir.0',\n",
              " 'dap_multiplexer.a354efee0f42.root.log.INFO.20210220-132604.49']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hLAz_h3bHT8"
      },
      "source": [
        "**Function for reading csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbNF7dGPOfwn"
      },
      "source": [
        "def showRawTrainingSamples(csv_filename):\n",
        "  df = pd.read_csv(Root+csv_filename)\n",
        "  print(csv_filename)\n",
        "  print(df.columns)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVMzjDTqOmD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04afbeea-77eb-4b9d-c267-c79bce276601"
      },
      "source": [
        "a_csv = showRawTrainingSamples('training-a.csv')\n",
        "b_csv = showRawTrainingSamples('training-b.csv')\n",
        "c_csv = showRawTrainingSamples('training-c.csv')\n",
        "d_csv = showRawTrainingSamples('training-d.csv')\n",
        "e_csv = showRawTrainingSamples('training-e.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training-a.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-b.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-c.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-d.csv\n",
            "Index(['original filename', 'scanid', 'digit', 'num', 'database name original',\n",
            "       'database name', 'filename'],\n",
            "      dtype='object')\n",
            "training-e.csv\n",
            "Index(['filename', 'original filename', 'districtid', 'institutionid',\n",
            "       'gender', 'age', 'datestamp', 'scanid', 'digit',\n",
            "       'database name original', 'database name'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-pDGmi5O22X"
      },
      "source": [
        "def dropColumns(csv_file):\n",
        "  csv_file = csv_file[['filename', 'digit']]\n",
        "  print(csv_file)\n",
        "  \n",
        "  print(csv_file.iloc[:5, :])   #First 5 Rows of the CSV File\n",
        "  print(\"=============================\")\n",
        "  return csv_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN8lCGl0O6jA"
      },
      "source": [
        "a_csv = dropColumns(a_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7DJMLA5Y1PC"
      },
      "source": [
        "**Merging all csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqPVPeS3PJGF",
        "outputId": "91904960-68f5-4811-809c-4dfafaf8e1bd"
      },
      "source": [
        "total_csv = [a_csv,c_csv, d_csv]\n",
        "merged_csv = pd.concat(total_csv)\n",
        "print(len(merged_csv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lDS31y2bVdn"
      },
      "source": [
        "TRAIN_PATH = 'train'\n",
        "os.mkdir(TRAIN_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a_dsVMnY9OG"
      },
      "source": [
        "**All images in one folder function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7--uYL0CbZgm"
      },
      "source": [
        "def processImages(folder_name):\n",
        "  src = Root + folder_name + '/'\n",
        "  dir_folders = os.listdir(src)\n",
        "  for dir_name in dir_folders:\n",
        "    file_name = os.path.join(src, dir_name)\n",
        "    if os.path.isfile(file_name):\n",
        "      shutil.copy(file_name, TRAIN_PATH)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6D8dTJgbiu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d961d8b5-f33f-4d85-e0ac-db9f19c472a0"
      },
      "source": [
        "processImages('training-a')\n",
        "print('A Done')\n",
        "#processImages('training-b')\n",
        "#print('B Done')\n",
        "processImages('training-c')\n",
        "print('C Done')\n",
        "processImages('training-d')\n",
        "print('D Done')\n",
        "#processImages('training-e')\n",
        "#print('E Done')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Done\n",
            "C Done\n",
            "D Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGhLVV2PRKFQ"
      },
      "source": [
        "### Hyperparameter initialazation\n",
        "Hyper parameter is predefined which is not changed during run time.But it is an important factor. we must be careful about choosing the hyperparameter value.\n",
        "Here batch size,number of iteration,input dimension,output dimension,learning rate are Hyperparameters.Even choosing the optimizer is also a hyperparameter.we chose loss Entropy instead of MSE here for better output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcJJuWTNPQBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc8a2cf-9f22-4775-86ec-ae67da0dfbe2"
      },
      "source": [
        "batch_size = 80\n",
        "num_iters = 2000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLTW54uPZLFy"
      },
      "source": [
        "**Function for preparing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r19g5vf4P3ZX"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpNjsus3ZgQK"
      },
      "source": [
        "**Resizing and normalizing  the trained and test images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04n4PmZsP8n-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80f2d68-d99b-4523-e9cf-c584c0ce7e89"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwY9qNMvZq7e"
      },
      "source": [
        "**Showing Image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9vbgGJeRcuSf",
        "outputId": "0b96da9a-4e3c-4ff4-9c76-80cb53dcacb8"
      },
      "source": [
        "\n",
        "show_img = train_data[250][0].numpy().reshape(28,28)\n",
        "plt.imshow(show_img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fedd204ab70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAO0klEQVR4nO3dX4he9Z3H8c/HpPFPWjD6LMOYhqRb4oUsbBKHWIgUl7LVeBOLKA1SsyCbXhhIoRcr7kW9lKV/6EUtpjUkXbqGQivmQnbrhoIUpDiGVKNh16wkNmFM+uBFDYjNTL57MSdlmsxzzuT5Pef5ncnv/YJhZs7vOed858x85pl5vuecnyNCAK5/N+QuAMB4EHagEIQdKARhBwpB2IFCrBznznq9Xqxfv36cuyyC7YFjTd2WunVHodRuT9NxrTsuKeuePn1a/X5/0Q0khd32A5J+KGmFpJ9GxLN1j1+/fr1ef/31ofd3ww38IbKYuuNy6dKlodcdhab9d3XbTZqOW9N4Xe0p695zzz2Dt1u71Rq2V0j6kaTtku6StNP2XcNuD0C7Un6tb5V0MiLej4g/SzokacdoygIwailhXyvpDws+P1Mt+yu2d9uetj3d7/cTdgcgRev/BEfEvoiYioipXq/X9u4ADJAS9rOS1i34/PPVMgAdlBL2NyRttP0F26skfV3S4dGUBWDUhm69RcSs7T2S/kvzrbf9EfFOSjGp7Yw6Ods0TVLbXylfW2prrs3WXtO2V66s//HN2fZr82e5bt26Hn1Snz0iXpH0Sso2AIwHZ6kAhSDsQCEIO1AIwg4UgrADhSDsQCHGej17qq5e0tjlXnWT1G0v19pSf5ZSv6cpl7jWqbvWnWd2oBCEHSgEYQcKQdiBQhB2oBCEHSjEWFtvthsvS6yTs/V2PV8iW6LU1lnq9tvadt0lrvyUAIUg7EAhCDtQCMIOFIKwA4Ug7EAhCDtQiGV1iWudnLdjbrtn22ZtbWvrUs5UOX9emvafcnksl7gCIOxAKQg7UAjCDhSCsAOFIOxAIQg7UIix9tkjorPXhaf0ylNvBf3pp5/Wjp84caJ2fOPGjQPHVq9eXbtu2+q+9uv1mvKc225tymbbpyR9LGlO0mxETKVsD0B7RvHM/g8R0R/BdgC0iP/ZgUKkhj0k/dr2m7Z3L/YA27ttT9ue7vf5AwDIJTXs90bEFknbJT1p+8tXPiAi9kXEVERM9Xq9xN0BGFZS2CPibPX+vKSXJG0dRVEARm/osNtebftzlz+W9FVJx0dVGIDRSnk1fkLSS1Vfb6Wk/4iI/0wpJve118NK7Rd/8MEHteOPPfZY7fiBAwcGjt1999216+Y85m2fc9HVczqktOM+7Nc1dNgj4n1Jfz/s+gDGa3k+lQK4ZoQdKARhBwpB2IFCEHagEGOfsjml5dBmuyKlTZPa4mm6xPXWW2+tHd+wYcPAsdTWWmpbMedtsFNu19y2HLfY5pkdKARhBwpB2IFCEHagEIQdKARhBwpB2IFCdGrK5py9z7Zva1znk08+qR2/+eaba8dvvPHGoffdZp9c6u4ltDm/303a2jfP7EAhCDtQCMIOFIKwA4Ug7EAhCDtQCMIOFKJTffbleivpVDMzM7XjN910U+34ihUrBo7Nzs7Wrpt6zFP69G1fa18ndZrtNvvwKduOiIFjZaYLKBBhBwpB2IFCEHagEIQdKARhBwpB2IFCjLXPHhFZ7yNeJ2fftGnK5nPnztWOHzp0aOBY07XyFy5cqB1v0tTHX7ly8I/YHXfcUbvu9u3ba8dvv/322vEc0yJ3WePRsL3f9nnbxxcsu832q7bfq96vabdMAKmW8qvvgKQHrlj2lKQjEbFR0pHqcwAd1hj2iHhN0kdXLN4h6WD18UFJD424LgAjNuw/NRMRcfmE7g8lTQx6oO3dtqdtT/f7/SF3ByBV8iteMX/m/cCz7yNiX0RMRcRUr9dL3R2AIQ0b9nO2JyWpen9+dCUBaMOwYT8saVf18S5JL4+mHABtaeyz235R0n2SerbPSPqOpGcl/cL2E5JOS3q0zSIvyzGn9VL2PTc31+q+jx49Wju+d+/egWOPPPJI7boPP/xw7fjExMCXYyQ137O+7p73ddfhS9Lq1atrx3NKvR6+rXVtDxxrDHtE7Bww9JVhCwIwfpwuCxSCsAOFIOxAIQg7UAjCDhSiU7eSXq6aWkhNNmzYUDt+//33147v2bNn4Njzzz9fu+7Fixdrxzdv3lw7nvP2312+hDWlNcetpAEkIexAIQg7UAjCDhSCsAOFIOxAIQg7UIhO9dmX6+17U6f33bJlS9L26y4zberhnzx5sna86fLd63Wa7ZxTNrfl+vxOAbgKYQcKQdiBQhB2oBCEHSgEYQcKQdiBQnSqz75ce7apPdemqYfrbhUtSc8999zQ+3788ceHXldq95bKOXvZOa93b2vfyzNdAK4ZYQcKQdiBQhB2oBCEHSgEYQcKQdiBQnSqz94kZcrmNvumqftuuu/8tm3basfXrl07cOyWW26pXXdycrJ2PPWe+CnfszbPu8h9PXqOc0oa92h7v+3zto8vWPaM7bO2j1VvD7ZbJoBUS/n1ckDSA4ss/0FEbKreXhltWQBGrTHsEfGapI/GUAuAFqX847DH9lvVn/lrBj3I9m7b07an+/1+wu4ApBg27D+W9EVJmyTNSPreoAdGxL6ImIqIqV6vN+TuAKQaKuwRcS4i5iLikqSfSNo62rIAjNpQYbe9sF/zNUnHBz0WQDc09tltvyjpPkk922ckfUfSfbY3SQpJpyR9cxTFtHltdJvarnvVqlW143feeefQ+87db67T5nFtu8ffVHvK+QfDagx7ROxcZPELLdQCoEXdfKoEMHKEHSgEYQcKQdiBQhB2oBBjv8S1rctUU1spXW5Btdly7Go7U0r/nnX5Etm62tr6urr7nQYwUoQdKARhBwpB2IFCEHagEIQdKARhBwox9j57W5cldrlPnrO2LvfRU+Xso+e8/Hbo7bayVQCdQ9iBQhB2oBCEHSgEYQcKQdiBQhB2oBBj7bNHhC5evDhwPHV64BTL+Xr3Om3XndITXs61LcfzF5ZfxQCGQtiBQhB2oBCEHSgEYQcKQdiBQhB2oBBj7bPbTuqld/k+4KVKOW45p0VOtRz78I0V2V5n+ze237X9ju291fLbbL9q+73q/Zr2ywUwrKX8+pmV9O2IuEvSlyQ9afsuSU9JOhIRGyUdqT4H0FGNYY+ImYg4Wn38saQTktZK2iHpYPWwg5IeaqtIAOmu6R8L2xskbZb0O0kTETFTDX0oaWLAOrttT9ue7vf7CaUCSLHksNv+rKRfSvpWRPxp4VhEhKRYbL2I2BcRUxEx1ev1kooFMLwlhd32ZzQf9J9HxK+qxedsT1bjk5LOt1MigFFobL3ZtqQXJJ2IiO8vGDosaZekZ6v3L6cWQ2utLKntq+V8CWuO26Ivpc++TdI3JL1t+1i17GnNh/wXtp+QdFrSo61UCGAkGsMeEb+V5AHDXxltOQDa0r3TfAC0grADhSDsQCEIO1AIwg4UYuxTNtfp8tTGbdbW9qWeKftO7Ufn/J7W7Tv3Jag5jgvP7EAhCDtQCMIOFIKwA4Ug7EAhCDtQCMIOFKJTffa5ubna8TandO5qP1hq97ruJqnbztnPTtl32+cftGX+9hOL45kdKARhBwpB2IFCEHagEIQdKARhBwpB2IFCjLXPHhG1vfSmPnqOe22PQ9P5BSnnH7R9rXzOXnaK3Nez12nrmHf3KwYwUoQdKARhBwpB2IFCEHagEIQdKARhBwqxlPnZ10n6maQJSSFpX0T80PYzkv5Z0h+rhz4dEa+0Vai0vHvpdVLOL1jOUr+fbc7P3ua+U9XVHhEDx5ZyUs2spG9HxFHbn5P0pu1Xq7EfRMR3r6VQAHksZX72GUkz1ccf2z4haW3bhQEYrWv6W8T2BkmbJf2uWrTH9lu299teM2Cd3banbU/3+/2kYgEMb8lht/1ZSb+U9K2I+JOkH0v6oqRNmn/m/95i60XEvoiYioipXq83gpIBDGNJYbf9Gc0H/ecR8StJiohzETEXEZck/UTS1vbKBJCqMeyev13lC5JORMT3FyyfXPCwr0k6PvryAIzKUl6N3ybpG5Letn2sWva0pJ22N2m+HXdK0jebNmS7ts2U81bSpUq9TXXO6aJTLgVdubL+R7/L04fXjdfdSnopr8b/VtJiW2i1pw5gtK7PszUAXIWwA4Ug7EAhCDtQCMIOFIKwA4Xo1JTNbfbRm3r4bUr9utrsZc/Oziatn/NW0ylSv+6chv156O5XBGCkCDtQCMIOFIKwA4Ug7EAhCDtQCMIOFMJ1t54d+c7sP0o6vWBRT1JXb0zX1dq6WpdEbcMaZW3rI+JvFhsYa9iv2rk9HRFT2Qqo0dXaulqXRG3DGldt/BkPFIKwA4XIHfZ9mfdfp6u1dbUuidqGNZbasv7PDmB8cj+zAxgTwg4UIkvYbT9g+39sn7T9VI4aBrF9yvbbto/Zns5cy37b520fX7DsNtuv2n6ver/oHHuZanvG9tnq2B2z/WCm2tbZ/o3td22/Y3tvtTzrsaupayzHbez/s9teIel/Jf2jpDOS3pC0MyLeHWshA9g+JWkqIrKfgGH7y5IuSPpZRPxdtezfJH0UEc9WvyjXRMS/dKS2ZyRdyD2NdzVb0eTCacYlPSTpn5Tx2NXU9ajGcNxyPLNvlXQyIt6PiD9LOiRpR4Y6Oi8iXpP00RWLd0g6WH18UPM/LGM3oLZOiIiZiDhaffyxpMvTjGc9djV1jUWOsK+V9IcFn59Rt+Z7D0m/tv2m7d25i1nERETMVB9/KGkiZzGLaJzGe5yumGa8M8dumOnPU/EC3dXujYgtkrZLerL6c7WTYv5/sC71Tpc0jfe4LDLN+F/kPHbDTn+eKkfYz0pat+Dzz1fLOiEizlbvz0t6Sd2bivrc5Rl0q/fnM9fzF12axnuxacbVgWOXc/rzHGF/Q9JG21+wvUrS1yUdzlDHVWyvrl44ke3Vkr6q7k1FfVjSrurjXZJezljLX+nKNN6DphlX5mOXffrziBj7m6QHNf+K/P9J+tccNQyo628l/b56eyd3bZJe1PyfdRc1/9rGE5Jul3RE0nuS/lvSbR2q7d8lvS3pLc0HazJTbfdq/k/0tyQdq94ezH3sauoay3HjdFmgELxABxSCsAOFIOxAIQg7UAjCDhSCsAOFIOxAIf4f3VZo6p33sZMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2JPsjgsj3Sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764ec951-f866-43de-e9e3-064491fbfae8"
      },
      "source": [
        "print(train_data[2052][0].numpy().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRGvn066Re5T"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 80\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 2000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 2000 \\div \\frac{54908}{80} = 2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2baCRrmRQReO",
        "outputId": "27950e76-3180-435b-fc4c-5292f1e276c5"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:618\n",
            "Test dataloader:69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdMbdEXMSTmc"
      },
      "source": [
        "###Logistic Regression\n",
        "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2AJ9YBAQ9qm"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits  = self.linear(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNYcu-PVRUxm",
        "outputId": "84a1da37-a78d-4c4a-cd06-f337e3ac1996"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = LogisticRegressionModel(input_size=input_dim,\n",
        "                                num_classes=output_dim)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isHs3Qh8IDTE"
      },
      "source": [
        "### Construct loss and optimizer (select from PyTorch API)\n",
        "\n",
        "Unlike linear regression, we do not use MSE here, we need Cross Entropy Loss to calculate our loss before we backpropagate and update our parameters.\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss() ` \n",
        "\n",
        "It does 2 things at the same time.\n",
        "\n",
        "1. Computes softmax **([Logistic or Sigmoid]/softmax function)**\n",
        "2. Computes Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luvejs5NRkcQ"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mwLjfCkIhA8"
      },
      "source": [
        "### Training: forward, loss, backward, step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpGCmHirSADV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9648f9b2-29fd-4d3f-a21e-ae3b6d3fc975"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        logits, probas = model(images) \n",
        "\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                logits, probas = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(probas, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            iteration_loss.append(loss.item())\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 15.98112964630127. Accuracy: 11.092896174863387\n",
            "Iteration: 1000. Loss: 20.191394805908203. Accuracy: 10.382513661202186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xw1M8WurA6N"
      },
      "source": [
        "**Setting 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDO-nPEZrNAP",
        "outputId": "75712556-f47c-4837-83f1-f1cbff4b6179"
      },
      "source": [
        "batch_size = 100\n",
        "num_iters = 3000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVDG5o6_wXW5"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp8CcSQLwe1A",
        "outputId": "6c41f000-91f4-4687-82c8-6910595d065e"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxpdx6H-SjP0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Fpms5CSrJR"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 3000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{54908}{100} = 5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acOkBfm4wrXf",
        "outputId": "d16596b8-3591-4085-8395-ba45e927d7d5"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:495\n",
            "Test dataloader:55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv6AU4X0wx1x"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits  = self.linear(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEh6zW7nw13U",
        "outputId": "fd891ec9-d378-4799-f397-05df8bc5bc1a"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = LogisticRegressionModel(input_size=input_dim,\n",
        "                                num_classes=output_dim)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdV-mh_Sw5S-"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyuMa_MHw88g",
        "outputId": "902ace65-03f0-4096-ae73-5c2ec2664eec"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        logits, probas = model(images) \n",
        "\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                logits, probas = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(probas, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            iteration_loss.append(loss.item())\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 25.246177673339844. Accuracy: 10.109289617486338\n",
            "Iteration: 1000. Loss: 27.081148147583008. Accuracy: 9.508196721311476\n",
            "Iteration: 1500. Loss: 21.35369873046875. Accuracy: 13.151183970856103\n",
            "Iteration: 2000. Loss: 27.233478546142578. Accuracy: 10.382513661202186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Y-vWE_zdTR"
      },
      "source": [
        "**Setting3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj-J3SbUzgz7",
        "outputId": "51a60869-7a8a-4a93-8326-987594bec770"
      },
      "source": [
        "batch_size = 100\n",
        "num_iters = 3000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3G82TJWzu2S"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAHhGyILzypm",
        "outputId": "ae27f692-3b71-4f23-e88f-2c20b44beaf5"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqLGejxKTDxJ"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 3000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{54908}{100} = 5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6dZpgP1z1yz",
        "outputId": "1ee699db-8585-4535-a53b-850aa42a68c8"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:495\n",
            "Test dataloader:55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4crTEElvz72x"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits  = self.linear(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hmOdeGDz_Fc",
        "outputId": "a51bec52-ba22-4c36-d363-922998c800d4"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = LogisticRegressionModel(input_size=input_dim,\n",
        "                                num_classes=output_dim)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujjAhBLQ0B6t"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQO0_wAL0FmT",
        "outputId": "9c6fa3a0-4830-4d9d-9d3f-6aeb8ba55271"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        logits, probas = model(images) \n",
        "\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                logits, probas = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(probas, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            iteration_loss.append(loss.item())\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.2938430309295654. Accuracy: 11.183970856102004\n",
            "Iteration: 1000. Loss: 2.2912189960479736. Accuracy: 10.273224043715848\n",
            "Iteration: 1500. Loss: 2.292511224746704. Accuracy: 12.568306010928962\n",
            "Iteration: 2000. Loss: 2.2945985794067383. Accuracy: 11.03825136612022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SKcwxSR138P"
      },
      "source": [
        "**Setting 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCIqslm3170n",
        "outputId": "c1ade948-31c9-481b-fe46-0a9a7632a108"
      },
      "source": [
        "batch_size = 100\n",
        "num_iters = 6000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHuFxViq2Yrn"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Ey8VE52cBr",
        "outputId": "4361db4b-e101-4f5b-b5ad-f401b9c15419"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBi9jAGlTPPN"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 6000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 6000 \\div \\frac{54908}{100} = 10$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FshaGhWZ2gN9",
        "outputId": "b9a0564d-f86f-4cb6-cd7b-e7489e9273b2"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:495\n",
            "Test dataloader:55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsjaLn8L2j6b"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits  = self.linear(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVv9_-8D2nTP",
        "outputId": "3719b0b4-9413-40d1-9094-3aa811283a35"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = LogisticRegressionModel(input_size=input_dim,\n",
        "                                num_classes=output_dim)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLzfDj1G2qP4"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E9PplV-2stH",
        "outputId": "b7f25c6c-5e45-4538-8163-d789e9b6688f"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        logits, probas = model(images) \n",
        "\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                logits, probas = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(probas, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            iteration_loss.append(loss.item())\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.2861320972442627. Accuracy: 10.036429872495447\n",
            "Iteration: 1000. Loss: 2.306555986404419. Accuracy: 11.001821493624773\n",
            "Iteration: 1500. Loss: 2.3008623123168945. Accuracy: 11.766848816029144\n",
            "Iteration: 2000. Loss: 2.2973828315734863. Accuracy: 15.22768670309654\n",
            "Iteration: 2500. Loss: 2.288130760192871. Accuracy: 15.482695810564662\n",
            "Iteration: 3000. Loss: 2.284956932067871. Accuracy: 14.681238615664846\n",
            "Iteration: 3500. Loss: 2.274024724960327. Accuracy: 17.959927140255008\n",
            "Iteration: 4000. Loss: 2.28307843208313. Accuracy: 18.59744990892532\n",
            "Iteration: 4500. Loss: 2.2701964378356934. Accuracy: 18.233151183970858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQKEZ6k-7_KA"
      },
      "source": [
        "**Setting 5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2jgf_q79n1",
        "outputId": "83d275bf-cf05-466d-9b3e-d2ca9b80c18a"
      },
      "source": [
        "batch_size = 32\n",
        "num_iters = 5000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbI5ZKHA-qSI"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU-gWpEf-vct",
        "outputId": "a96fc38e-94ad-42c7-97e7-f921b5f6d144"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJy6J14PTfxQ"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 32\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 5000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 5000 \\div \\frac{54908}{32} = 2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d11V3t95-zyD",
        "outputId": "3b89c152-af8b-495f-ea77-8732061bc4b3"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:1545\n",
            "Test dataloader:172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVbg4ntN-4r4"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits  = self.linear(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5X_cO3I-8Jp",
        "outputId": "ba637d3d-43e6-49ad-9cc8-a3ee24de898a"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = LogisticRegressionModel(input_size=input_dim,\n",
        "                                num_classes=output_dim)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfuVKqTX-_mC"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RA1SD_2_LOY",
        "outputId": "6e7ed1a0-969b-46be-fea7-841c7210b211"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        logits, probas = model(images) \n",
        "\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                logits, probas = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(probas, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            iteration_loss.append(loss.item())\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.2974557876586914. Accuracy: 10.29143897996357\n",
            "Iteration: 1000. Loss: 2.2858009338378906. Accuracy: 10.52823315118397\n",
            "Iteration: 1500. Loss: 2.345881938934326. Accuracy: 10.145719489981785\n",
            "Iteration: 2000. Loss: 2.3118555545806885. Accuracy: 11.675774134790528\n",
            "Iteration: 2500. Loss: 2.2939467430114746. Accuracy: 10.947176684881603\n",
            "Iteration: 3000. Loss: 2.2994260787963867. Accuracy: 10.510018214936247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBkjZXK-AJLG"
      },
      "source": [
        "**Setting 6**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42uRALdbAMv7",
        "outputId": "9b288d06-2a54-4cb5-b520-fa648f0122c1"
      },
      "source": [
        "batch_size = 200\n",
        "num_iters = 5000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY6T37a8AW9A"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHdDRghKAalD",
        "outputId": "74faab14-2c77-494c-941d-ea8568345b4a"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdlekBftTq1j"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 200\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 5000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 5000 \\div \\frac{54908}{200} = 18$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVWl_N7AAfIs",
        "outputId": "51e32d08-d958-4bef-e7f9-29c055cf34aa"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:248\n",
            "Test dataloader:28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBLTctmAlGD"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits  = self.linear(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcUxWXFGAojg",
        "outputId": "ccc06f03-0f98-4737-cc9a-90dfd41ed0c1"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = LogisticRegressionModel(input_size=input_dim,\n",
        "                                num_classes=output_dim)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xaXFkvsAspJ"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAFkUyA3AwpZ",
        "outputId": "f5ca135b-ded6-446c-8e15-706a935197a6"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        logits, probas = model(images) \n",
        "\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                logits, probas = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(probas, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            iteration_loss.append(loss.item())\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.3067924976348877. Accuracy: 8.797814207650273\n",
            "Iteration: 1000. Loss: 2.3036117553710938. Accuracy: 10.29143897996357\n",
            "Iteration: 1500. Loss: 2.295410394668579. Accuracy: 10.856102003642988\n",
            "Iteration: 2000. Loss: 2.2986769676208496. Accuracy: 14.82695810564663\n",
            "Iteration: 2500. Loss: 2.2884795665740967. Accuracy: 12.750455373406194\n",
            "Iteration: 3000. Loss: 2.287173271179199. Accuracy: 18.37887067395264\n",
            "Iteration: 3500. Loss: 2.29063081741333. Accuracy: 20.036429872495447\n",
            "Iteration: 4000. Loss: 2.281463384628296. Accuracy: 19.799635701275047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7qqfKoCEpBV"
      },
      "source": [
        "**Setting 7**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfSHCx4KEuQx",
        "outputId": "9a49e968-dd82-40bc-b354-1670db48f0b0"
      },
      "source": [
        "batch_size = 200\n",
        "num_iters = 10000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPqiCVryEx8k"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed1Gj1jXE4bB",
        "outputId": "94063e36-379c-4d8f-f3a1-4ff929c7a76f"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVPn7BN0T2Sh"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 200\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 10000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 10000 \\div \\frac{54908}{200} = 36$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pmeoBZ8E99s",
        "outputId": "2a0b7db5-07f8-4d89-e3d7-bd302d0615df"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:248\n",
            "Test dataloader:28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkupygQoFEAC"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits  = self.linear(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OM7b9QdFFyT",
        "outputId": "095ab82f-789b-494f-f7cd-971d7f505ec5"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = LogisticRegressionModel(input_size=input_dim,\n",
        "                                num_classes=output_dim)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXemI1k7FKVq"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yZ1YALnFMul",
        "outputId": "c3036595-ff88-4cb4-9c07-917f7773aedd"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        logits, probas = model(images) \n",
        "\n",
        "        # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                logits, probas = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(probas, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            iteration_loss.append(loss.item())\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.2988016605377197. Accuracy: 10.728597449908925\n",
            "Iteration: 1000. Loss: 2.301626443862915. Accuracy: 9.41712204007286\n",
            "Iteration: 1500. Loss: 2.297241449356079. Accuracy: 10.491803278688524\n",
            "Iteration: 2000. Loss: 2.291654109954834. Accuracy: 9.508196721311476\n",
            "Iteration: 2500. Loss: 2.294448137283325. Accuracy: 10.163934426229508\n",
            "Iteration: 3000. Loss: 2.291027545928955. Accuracy: 12.021857923497267\n",
            "Iteration: 3500. Loss: 2.2889580726623535. Accuracy: 14.681238615664846\n",
            "Iteration: 4000. Loss: 2.285304069519043. Accuracy: 16.994535519125684\n",
            "Iteration: 4500. Loss: 2.2855584621429443. Accuracy: 16.97632058287796\n",
            "Iteration: 5000. Loss: 2.2752764225006104. Accuracy: 25.00910746812386\n",
            "Iteration: 5500. Loss: 2.2701685428619385. Accuracy: 26.994535519125684\n",
            "Iteration: 6000. Loss: 2.276379346847534. Accuracy: 26.939890710382514\n",
            "Iteration: 6500. Loss: 2.2722508907318115. Accuracy: 28.306010928961747\n",
            "Iteration: 7000. Loss: 2.259967565536499. Accuracy: 28.561020036429873\n",
            "Iteration: 7500. Loss: 2.2639589309692383. Accuracy: 27.92349726775956\n",
            "Iteration: 8000. Loss: 2.2596800327301025. Accuracy: 26.502732240437158\n",
            "Iteration: 8500. Loss: 2.261944055557251. Accuracy: 27.194899817850636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c3X_QK7SeRA"
      },
      "source": [
        "\n",
        "###Result\n",
        "|Setting| Batch Size       | No. of iteration     | Learning Rate     |\n",
        "| :------------- | :----------: | -----------: |:------------- |\n",
        "|           |\n",
        "|1|  80 | 2000   | .1    |\n",
        "|2| 100   | 3000 | .1|\n",
        "|3| 100|3000 |.001 |\n",
        "|4| 100|6000 |.001 |\n",
        "|5|32|5000|.001|\n",
        "|6|200|5000|.001|\n",
        "|7|200|10000|.001||\n",
        "\n",
        "In first setting batch size is 80, no. of iteration is 2000 and learning rate is .1 the accuracy is 10-11% .In setting 2 batch size was updated  others were remained same. but there was no increasing of accuracy.in setting 3 i kept batch size and  no. of iteration same and learning rate was changed from .1 to .001  still there is no improvement of the accuracy.but in setting 4 when i changed the no. of iteration from 3000 to 6000 and others were kept same then improvement in the accuracy was shown.in setting 6 and 7 batch size was decreased very less and after that setting batch size was increased very high. but there is no difference was visible in term of accuracy.\n",
        "At last in last setting  no. of iteration was 10000 and accuracy was the best among all of the settings.\n",
        "So,in my model higher no. of iteration is increasing the accuracy . \n",
        "\n"
      ]
    }
  ]
}