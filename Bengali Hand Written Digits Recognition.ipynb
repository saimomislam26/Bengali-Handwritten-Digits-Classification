{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "160204011_problem#1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0641Q1HBwt8W"
      },
      "source": [
        "###NumtaDB Bangla handwritten Digits\n",
        "Bengali Handwritten Digits recognization can be solved using many techniques.  I will use neural network for solving this problem\n",
        " \n",
        "the main motive is to make the model recognize the hand written digit accurately.,changing the hyperparameter to see for which combination the accuracy is better.\n",
        "\n",
        "\n",
        "**Numta DB** is a database.This database contains handwritten digits (0 through 9) in bengali, and can provide a baseline for testing image processing systems.\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1k5iKYOTrJmuSRPbWazZyBboaUPWabcmR\" width=\"400\">\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0ZO5nuUYO41"
      },
      "source": [
        "**Importing all necessary library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5pshGFhEmFy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from os import path\n",
        "from torchvision import  models\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grCZpAJqExjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33bf9be-65b2-42b4-b49d-b7687a76190b"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "lhQeA6c3E1DK",
        "outputId": "ea47830c-5603-4856-977e-4c5b2baf62ec"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-078fbf2e-5a94-4ef0-8b87-9b02559ab15b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-078fbf2e-5a94-4ef0-8b87-9b02559ab15b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"saimom\",\"key\":\"836f2f1447e07bed180067c9bd62a2ee\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhFaN-h9E4zB",
        "outputId": "99f5f55a-ba43-473f-e00d-4dbf69780985"
      },
      "source": [
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLWAbMwYcSn"
      },
      "source": [
        "**importing numtaDB dataset from kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrXfOM6AJclC",
        "outputId": "85c68ba6-9995-47ce-f5d3-85298ddfc436"
      },
      "source": [
        "!kaggle datasets download -d BengaliAI/numta -p /content/gdrive/My\\ Drive/Softcom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading numta.zip to /content/gdrive/My Drive/Softcom\n",
            "100% 1.90G/1.91G [00:21<00:00, 84.2MB/s]\n",
            "100% 1.91G/1.91G [00:21<00:00, 97.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP_7dIwzYnXB"
      },
      "source": [
        "**unzipping numta.zip dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56_Z7kv7F2hL",
        "outputId": "4455855f-a8ab-40ce-8d4e-00338a2730cd"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"/content/gdrive/My Drive/Softcom/numta.zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkDp94RSGHok",
        "outputId": "1ce4810e-e3ab-4b45-d001-aa534ff3dce5"
      },
      "source": [
        "from google.colab import drive #mounted all file and folder from google drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0yg4xSlZzGL"
      },
      "source": [
        "**changing the directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2c81uYEHL76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae354138-7872-4e2a-cf65-d0ce090edfc2"
      },
      "source": [
        "root_path = '/content/gdrive/My Drive/Softcom/'\n",
        "Root = '/content/'\n",
        "os.listdir(Root)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'training-c',\n",
              " 'training-d',\n",
              " 'training-c.csv',\n",
              " 'testing-b',\n",
              " 'testing-all-corrected',\n",
              " 'training-a.csv',\n",
              " 'testing-e',\n",
              " 'testing-f',\n",
              " 'training-e.csv',\n",
              " 'gdrive',\n",
              " 'testing-c',\n",
              " 'kaggle.json',\n",
              " 'testing-a',\n",
              " 'testing-d',\n",
              " 'training-a',\n",
              " 'testing-augc',\n",
              " 'training-b',\n",
              " 'training-e',\n",
              " 'training-b.csv',\n",
              " 'training-d.csv',\n",
              " 'testing-auga',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hLAz_h3bHT8"
      },
      "source": [
        "**Function for reading csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbNF7dGPOfwn"
      },
      "source": [
        "def showRawTrainingSamples(csv_filename):\n",
        "  df = pd.read_csv(Root+csv_filename)\n",
        "  print(csv_filename)\n",
        "  print(df.columns)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVMzjDTqOmD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8842da89-1635-4fd1-f4cb-0f0449406a9a"
      },
      "source": [
        "a_csv = showRawTrainingSamples('training-a.csv')\n",
        "b_csv = showRawTrainingSamples('training-b.csv')\n",
        "c_csv = showRawTrainingSamples('training-c.csv')\n",
        "d_csv = showRawTrainingSamples('training-d.csv')\n",
        "e_csv = showRawTrainingSamples('training-e.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training-a.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-b.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-c.csv\n",
            "Index(['filename', 'original filename', 'scanid', 'digit',\n",
            "       'database name original', 'contributing team', 'database name'],\n",
            "      dtype='object')\n",
            "training-d.csv\n",
            "Index(['original filename', 'scanid', 'digit', 'num', 'database name original',\n",
            "       'database name', 'filename'],\n",
            "      dtype='object')\n",
            "training-e.csv\n",
            "Index(['filename', 'original filename', 'districtid', 'institutionid',\n",
            "       'gender', 'age', 'datestamp', 'scanid', 'digit',\n",
            "       'database name original', 'database name'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-pDGmi5O22X"
      },
      "source": [
        "def dropColumns(csv_file):\n",
        "  csv_file = csv_file[['filename', 'digit']]\n",
        "  print(csv_file)\n",
        "  \n",
        "  print(csv_file.iloc[:5, :])   #First 5 Rows of the CSV File\n",
        "  print(\"=============================\")\n",
        "  return csv_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN8lCGl0O6jA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a1359d-6d81-4d08-bfab-9396ae6d81dc"
      },
      "source": [
        "a_csv = dropColumns(a_csv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         filename  digit\n",
            "0      a00000.png      5\n",
            "1      a00001.png      3\n",
            "2      a00002.png      1\n",
            "3      a00003.png      7\n",
            "4      a00004.png      0\n",
            "...           ...    ...\n",
            "19697  a19697.png      4\n",
            "19698  a19698.png      3\n",
            "19699  a19699.png      8\n",
            "19700  a19700.png      3\n",
            "19701  a19701.png      8\n",
            "\n",
            "[19702 rows x 2 columns]\n",
            "     filename  digit\n",
            "0  a00000.png      5\n",
            "1  a00001.png      3\n",
            "2  a00002.png      1\n",
            "3  a00003.png      7\n",
            "4  a00004.png      0\n",
            "=============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7DJMLA5Y1PC"
      },
      "source": [
        "**Merging all csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqPVPeS3PJGF",
        "outputId": "33e66d0c-ffa5-4d06-8a08-393075e28bac"
      },
      "source": [
        "total_csv = [a_csv,c_csv, d_csv]\n",
        "merged_csv = pd.concat(total_csv)\n",
        "print(len(merged_csv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lDS31y2bVdn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "83570e52-1154-4982-9560-e6cc8fbd0c72"
      },
      "source": [
        "TRAIN_PATH = 'train'\n",
        "os.mkdir(TRAIN_PATH)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c63973311797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTRAIN_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a_dsVMnY9OG"
      },
      "source": [
        "**All images in one folder function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7--uYL0CbZgm"
      },
      "source": [
        "def processImages(folder_name):\n",
        "  src = Root + folder_name + '/'\n",
        "  dir_folders = os.listdir(src)\n",
        "  for dir_name in dir_folders:\n",
        "    file_name = os.path.join(src, dir_name)\n",
        "    if os.path.isfile(file_name):\n",
        "      shutil.copy(file_name, TRAIN_PATH)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6D8dTJgbiu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c2e0fd-731b-4e2f-8fce-325574e8d7e9"
      },
      "source": [
        "processImages('training-a')\n",
        "print('A Done')\n",
        "#processImages('training-b')\n",
        "#print('B Done')\n",
        "processImages('training-c')\n",
        "print('C Done')\n",
        "processImages('training-d')\n",
        "print('D Done')\n",
        "#processImages('training-e')\n",
        "#print('E Done')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Done\n",
            "C Done\n",
            "D Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGhLVV2PRKFQ"
      },
      "source": [
        "### Hyperparameter initialazation\n",
        "Hyper parameter is predefined which is not changed during run time.But it is an important factor. we must be careful about choosing the hyperparameter value.\n",
        "Here batch size,number of iteration,input dimension,output dimension,learning rate are Hyperparameters.Even choosing the optimizer is also a hyperparameter.we chose loss Entropy instead of MSE here for better output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcJJuWTNPQBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4740b83c-8d2f-4c43-8e55-98a7c7321fb4"
      },
      "source": [
        "batch_size = 80\n",
        "num_iters = 2000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "num_hidden = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLTW54uPZLFy"
      },
      "source": [
        "**Function for preparing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r19g5vf4P3ZX"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpNjsus3ZgQK"
      },
      "source": [
        "**Resizing and normalizing  the trained and test images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04n4PmZsP8n-"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwY9qNMvZq7e"
      },
      "source": [
        "**Showing Image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9vbgGJeRcuSf",
        "outputId": "b57ab135-08ab-43f5-f1ae-5e010b9fce64"
      },
      "source": [
        "\n",
        "show_img = train_data[250][0].numpy().reshape(28,28)\n",
        "plt.imshow(show_img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7faa20ecdfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2JPsjgsj3Sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51d1b02-fa26-4f4c-d276-42ce5508bc38"
      },
      "source": [
        "print(train_data[2052][0].numpy().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRGvn066Re5T"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 80\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 2000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 2000 \\div \\frac{54908}{80} = 2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2baCRrmRQReO",
        "outputId": "a228ff8b-cb7c-443e-dbe4-98818de7882c"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:618\n",
            "Test dataloader:69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK3JEiUaE7Rd",
        "outputId": "989def43-3a04-44d8-8755-a212b1e8acbe"
      },
      "source": [
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepNeuralNetworkModel(\n",
              "  (linear_1): Linear(in_features=784, out_features=100, bias=True)\n",
              "  (relu_1): ReLU()\n",
              "  (linear_2): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (relu_2): ReLU()\n",
              "  (linear_3): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (relu_3): ReLU()\n",
              "  (linear_out): Linear(in_features=100, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdMbdEXMSTmc"
      },
      "source": [
        "###Neural Network\n",
        "A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of real biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be âˆ’1 and 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2AJ9YBAQ9qm"
      },
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits  = self.linear(x)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNYcu-PVRUxm",
        "outputId": "84a1da37-a78d-4c4a-cd06-f337e3ac1996"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = LogisticRegressionModel(input_size=input_dim,\n",
        "                                num_classes=output_dim)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel(\n",
              "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isHs3Qh8IDTE"
      },
      "source": [
        "### Construct loss and optimizer (select from PyTorch API)\n",
        "\n",
        "Unlike linear regression, we do not use MSE here, we need Cross Entropy Loss to calculate our loss before we backpropagate and update our parameters.\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss() ` \n",
        "\n",
        "It does 2 things at the same time.\n",
        "\n",
        "1. Computes softmax **([Logistic or Sigmoid]/softmax function)**\n",
        "2. Computes Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6Z_ot_uGaTC"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luvejs5NRkcQ"
      },
      "source": [
        "# INSTANTIATE OPTIMIZER CLASS\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mwLjfCkIhA8"
      },
      "source": [
        "### Training: forward, loss, backward, step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpGCmHirSADV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08a0ccc-ed46-4026-feab-67ab18de12b4"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.2991738319396973. Accuracy: 10.564663023679417\n",
            "Iteration: 1000. Loss: 2.2986435890197754. Accuracy: 13.66120218579235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xw1M8WurA6N"
      },
      "source": [
        "**Setting 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDO-nPEZrNAP",
        "outputId": "d83f344b-2017-4e7d-b3ca-f4a9d43f5c50"
      },
      "source": [
        "batch_size = 100\n",
        "num_iters = 3000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "num_hidden = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVDG5o6_wXW5"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp8CcSQLwe1A",
        "outputId": "86390f9f-e450-4d07-ef58-f2bb19baf498"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxpdx6H-SjP0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Fpms5CSrJR"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 3000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{54908}{100} = 5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acOkBfm4wrXf",
        "outputId": "dabe45af-c55e-4807-bd77-a9e50b5d9920"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:495\n",
            "Test dataloader:55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcDtQ2FJIUMh"
      },
      "source": [
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyuMa_MHw88g",
        "outputId": "55bec313-54e1-4737-b2f4-6516e999be21"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.2383480072021484. Accuracy: 19.07103825136612\n",
            "Iteration: 1000. Loss: 2.2387614250183105. Accuracy: 13.114754098360656\n",
            "Iteration: 1500. Loss: 2.199225425720215. Accuracy: 10.236794171220401\n",
            "Iteration: 2000. Loss: 2.2497000694274902. Accuracy: 13.242258652094717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Y-vWE_zdTR"
      },
      "source": [
        "**Setting3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj-J3SbUzgz7",
        "outputId": "b16a4d40-78b1-42db-8d98-b1d0c1389065"
      },
      "source": [
        "batch_size = 100\n",
        "num_iters = 3000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "num_hidden = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3G82TJWzu2S"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAHhGyILzypm",
        "outputId": "4103b2f3-a092-48f9-f4f5-d8b2efda9778"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqLGejxKTDxJ"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 3000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 3000 \\div \\frac{54908}{100} = 5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6dZpgP1z1yz",
        "outputId": "cedc6cd8-da3a-4f24-be25-9133bc0278b2"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:495\n",
            "Test dataloader:55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4crTEElvz72x"
      },
      "source": [
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQO0_wAL0FmT",
        "outputId": "409bb652-059f-42cf-b239-c0f042ebb0fc"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.3070762157440186. Accuracy: 10.218579234972678\n",
            "Iteration: 1000. Loss: 2.3027970790863037. Accuracy: 10.218579234972678\n",
            "Iteration: 1500. Loss: 2.2985239028930664. Accuracy: 10.218579234972678\n",
            "Iteration: 2000. Loss: 2.3015286922454834. Accuracy: 10.218579234972678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SKcwxSR138P"
      },
      "source": [
        "**Setting 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCIqslm3170n",
        "outputId": "aef78ee6-cbef-4e00-f3e1-c5f71a2879cd"
      },
      "source": [
        "batch_size = 100\n",
        "num_iters = 6000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "num_hidden = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHuFxViq2Yrn"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Ey8VE52cBr",
        "outputId": "c1cb1327-4443-495c-b268-90e916f50814"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBi9jAGlTPPN"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 100\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 6000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 6000 \\div \\frac{54908}{100} = 10$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FshaGhWZ2gN9",
        "outputId": "4539902e-4a5e-4eab-acc5-610ea660f6cb"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:495\n",
            "Test dataloader:55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsjaLn8L2j6b"
      },
      "source": [
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E9PplV-2stH",
        "outputId": "ed523697-8843-47f6-fbfb-a0f6d602362b"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.296241044998169. Accuracy: 10.473588342440802\n",
            "Iteration: 1000. Loss: 2.294959306716919. Accuracy: 10.473588342440802\n",
            "Iteration: 1500. Loss: 2.3031046390533447. Accuracy: 10.473588342440802\n",
            "Iteration: 2000. Loss: 2.304809331893921. Accuracy: 10.309653916211293\n",
            "Iteration: 2500. Loss: 2.300670862197876. Accuracy: 10.400728597449909\n",
            "Iteration: 3000. Loss: 2.2974512577056885. Accuracy: 10.34608378870674\n",
            "Iteration: 3500. Loss: 2.30478572845459. Accuracy: 9.70856102003643\n",
            "Iteration: 4000. Loss: 2.301753520965576. Accuracy: 9.617486338797814\n",
            "Iteration: 4500. Loss: 2.302274703979492. Accuracy: 9.562841530054644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQKEZ6k-7_KA"
      },
      "source": [
        "**Setting 5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2jgf_q79n1",
        "outputId": "e6bc010d-daef-42a7-de03-a9f677cef405"
      },
      "source": [
        "batch_size = 32\n",
        "num_iters = 5000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "num_hidden = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbI5ZKHA-qSI"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU-gWpEf-vct",
        "outputId": "4499075a-25a3-4ce9-bc8b-b4d4c6a7505f"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJy6J14PTfxQ"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 32\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 5000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 5000 \\div \\frac{54908}{32} = 2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d11V3t95-zyD",
        "outputId": "aeeee9c6-6852-42bc-ec34-c9b8737870e9"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:1545\n",
            "Test dataloader:172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVbg4ntN-4r4"
      },
      "source": [
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RA1SD_2_LOY",
        "outputId": "388cb1d6-f4c1-4573-d6d2-d7ff4d660165"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.3004209995269775. Accuracy: 10.163934426229508\n",
            "Iteration: 1000. Loss: 2.3158860206604004. Accuracy: 9.544626593806921\n",
            "Iteration: 1500. Loss: 2.295912265777588. Accuracy: 9.781420765027322\n",
            "Iteration: 2000. Loss: 2.305145263671875. Accuracy: 10.0\n",
            "Iteration: 2500. Loss: 2.304875612258911. Accuracy: 10.145719489981785\n",
            "Iteration: 3000. Loss: 2.298393487930298. Accuracy: 10.072859744990893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBkjZXK-AJLG"
      },
      "source": [
        "**Setting 6**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42uRALdbAMv7",
        "outputId": "3588eef6-f94d-49e5-b813-1bac50975be7"
      },
      "source": [
        "batch_size = 200\n",
        "num_iters = 5000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "output_dim = 10\n",
        "num_hidden = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY6T37a8AW9A"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, df, root, transform=None):\n",
        "        self.data = df\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = self.data.iloc[index]\n",
        "        \n",
        "        path = self.root + \"/\" +  item[0]\n",
        "        image = Image.open(path).convert('L')\n",
        "        label = item[1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHdDRghKAalD",
        "outputId": "5e609eda-c50a-4df2-bad7-cb45d4efcde3"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    #transforms.Scale(28,28),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(28),\n",
        "        #transforms.Scale(28,28),\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data  = Dataset(merged_csv,TRAIN_PATH, train_transform)\n",
        "test_data = Dataset(merged_csv,TRAIN_PATH, test_transform)\n",
        "\n",
        "\n",
        "print(\"Trainig Samples: \",len(train_data))\n",
        "print(\"Test Samples: \",len(test_data))\n",
        "#print(train_data[50])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainig Samples:  54908\n",
            "Test Samples:  54908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdlekBftTq1j"
      },
      "source": [
        "splitting the training set to **90%** and test set to **10%**. That means a **90:10** ratio. \n",
        "- **totaldata:** 54908\n",
        "- **minibatch:** 200\n",
        "  - Number of examples in **1** iteration\n",
        "\n",
        "- **iterations:** 5000\n",
        "  - *1 iteration: one mini-batch forward & backward pass. That means a parameter (wights and biases) update.*\n",
        "\n",
        "- **epochs**\n",
        "  - 1 epoch: running through the whole dataset once\n",
        "  - $epochs = iterations \\div \\frac{totaldata}{minibatch} = 5000 \\div \\frac{54908}{200} = 18$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVWl_N7AAfIs",
        "outputId": "7587f68f-4c5a-48d4-fb95-6bc6f0d62004"
      },
      "source": [
        "num_train = len(train_data)\n",
        "\n",
        "# split data 10% for testing\n",
        "test_size = 0.1\n",
        "\n",
        "# mix data\n",
        "# index of num of train\n",
        "indices = list(range(num_train))\n",
        "# random the index\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(test_size * num_train))\n",
        "# divied into two part\n",
        "train_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "num_epochs = num_iters / (len(train_data) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# prepare loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size,\n",
        "    sampler = train_idx\n",
        "    )\n",
        "#i faced the sampler and shuffle problem\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size,\n",
        "    sampler = test_idx\n",
        "    )\n",
        "\n",
        "print(\"Train dataloader:{}\".format(len(train_loader)))\n",
        "print(\"Test dataloader:{}\".format(len(test_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataloader:248\n",
            "Test dataloader:28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBLTctmAlGD"
      },
      "source": [
        "class DeepNeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer: 784 --> 100\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        ### 2nd hidden layer: 100 --> 100\n",
        "        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        self.relu_2 = nn.ReLU()\n",
        "\n",
        "        ### 3rd hidden layer: 100 --> 100\n",
        "        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "\n",
        "        ### Output layer: 100 --> 10\n",
        "        self.linear_out = nn.Linear(num_hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### 1st hidden layer\n",
        "        out  = self.linear_1(x)\n",
        "        ### Non-linearity in 1st hidden layer\n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        ### 2nd hidden layer\n",
        "        out  = self.linear_2(out)\n",
        "        ### Non-linearity in 2nd hidden layer\n",
        "        out = self.relu_2(out)\n",
        "\n",
        "        ### 3rd hidden layer\n",
        "        out  = self.linear_3(out)\n",
        "        ### Non-linearity in 3rd hidden layer\n",
        "        out = self.relu_3(out)\n",
        "        \n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas\n",
        "\n",
        "# INSTANTIATE MODEL CLASS\n",
        "\n",
        "model = DeepNeuralNetworkModel(input_size = input_dim,\n",
        "                               num_classes = output_dim,\n",
        "                               num_hidden = num_hidden)\n",
        "# To enable GPU\n",
        "model.to(device)\n",
        "\n",
        "# INSTANTIATE LOSS & OPTIMIZER CLASS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAFkUyA3AwpZ",
        "outputId": "14804344-55ea-493b-f359-3133d914f4d5"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iteration_loss = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images) \n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "               \n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum() \n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 2.3004629611968994. Accuracy: 9.380692167577413\n",
            "Iteration: 1000. Loss: 2.3115832805633545. Accuracy: 9.380692167577413\n",
            "Iteration: 1500. Loss: 2.3055124282836914. Accuracy: 9.380692167577413\n",
            "Iteration: 2000. Loss: 2.302356004714966. Accuracy: 9.380692167577413\n",
            "Iteration: 2500. Loss: 2.3051397800445557. Accuracy: 9.380692167577413\n",
            "Iteration: 3000. Loss: 2.3082175254821777. Accuracy: 9.380692167577413\n",
            "Iteration: 3500. Loss: 2.3020873069763184. Accuracy: 9.380692167577413\n",
            "Iteration: 4000. Loss: 2.3022334575653076. Accuracy: 9.380692167577413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c3X_QK7SeRA"
      },
      "source": [
        "\n",
        "###Result\n",
        "|Setting| Batch Size       | No. of iteration     | Learning Rate     |\n",
        "| :------------- | :----------: | -----------: |:------------- |\n",
        "|           |\n",
        "|1|  80 | 2000   | .1    |\n",
        "|2| 100   | 3000 | .1|\n",
        "|3| 100|3000 |.001 |\n",
        "|4| 100|6000 |.001 |\n",
        "|5|32|5000|.001|\n",
        "|6|200|5000|.001|\n",
        "|\n",
        "\n",
        "In first setting batch size is 80, no. of iteration is 2000 and learning rate is .1 the accuracy is 13% .In setting 2 batch size was updated  others were remained same. accuracy increased a bit.accuracy was 19%.in setting 3 i kept batch size and  no. of iteration same and learning rate was changed from .1 to .001  still there is no improvement of the accuracy.accuracy was 10%.in setting 4 when i changed the no. of iteration from 3000 to 6000 and others were kept same accuracy was still 10%.in setting 5 and 6 batch size was decreased very less and after that setting batch size was increased very high. but there is no difference was visible in term of accuracy.\n",
        "\n",
        "So,in my model higher no. of iteration and batch size is increasing the accuracy . \n",
        "\n"
      ]
    }
  ]
}